{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "# Basics of tensorflow ?\n",
    "1. ### Fundamentals of tensorflow \n",
    "    1. Data structure in TF\n",
    "    1. Data flow graphs \n",
    "    1. tensorflow sessions \n",
    "    1. tensorflow placeholder\n",
    "    1. tensorflow variables    \n",
    "    \n",
    "1. ### Forward propagation \n",
    "    1. all types of loss function \n",
    "    1. softmax vs cross entropy is same ? quick trick question let's who can explain me the difference \n",
    "    1. gradient descent \n",
    "    1. optimzers\n",
    "    1. activation functions \n",
    "    1. Batching  and it's types \n",
    "1. ### tensorflow for classification and Regression\n",
    "    1. Regularisation \n",
    "    1. Batch Normalization \n",
    "    1. Advance training algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data structure\n",
    "   * In Tensorflow we have tensor(fancy name for arrays with arbitary dimensions) for representing data and they array of varying dimensions\n",
    "   * one dimensional array is called as vector in tensorflow it is refferred is 1d tensor same as for 2 dimensional space is called 2d Tensor even for cube it is refferred as 3d tensor so basically dimension is property that describe every tensor\n",
    "   \n",
    "   \n",
    "![](https://miro.medium.com/max/1752/1*RiQ5LRM0CLfJToI3fk7VLA.png**)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1. now one big question for all of is there any difference between data type vs tensorflow data type if yes then what is that and if no then why we can't define it like other "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataflow Graph\n",
    "*    basically to carry out operation in tensorflow it is done with the help of dataflow graphs / diagrams     (internally) whatever constants variables optimizer weights biases you define that flow within graphs and operations are performed on them when we define sessions and intiate them explicitly\n",
    "\n",
    "*    graph contains 2 things node and edges , on node (operations performed) on edge (data-flows)\n",
    "\n",
    "![](https://miro.medium.com/max/1200/1*vPb9E0Yd1QUAD0oFmAgaOw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### as you can easily see internally in tensorflow how edges and node are used to carry out dataflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataflow graph of NN \n",
    "\n",
    "![](https://images.techhive.com/images/article/2016/10/tensorflow-data-flow-100685891-large.idge.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2 Why dataflow graph came up as idea to tensorflow developers because when we have numpy and we can still build the model using numpy yes we can then why dataflow graphs? ask yourself or wait for live session "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Session (switches to control your dataflow graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### so big picture in small words see in any tensorflow programme consists of 2 parts\n",
    "1. Data-flow (series of tensorflow operations arranged of graph nodes)\n",
    "1. Running computation graph in a session (encapsulates control and tensorflow runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of the multiplication: 25 \n"
     ]
    }
   ],
   "source": [
    "# Create an empty graph, that will be filled with operation nodes later\n",
    "main_graph=tf.Graph()\n",
    "\n",
    "\n",
    "# Register this graph as default graph. \n",
    "# All operations within this context will become operation nodes in this graph\n",
    "with main_graph.as_default():\n",
    "    \n",
    "    # Create two constants of value 5\n",
    "    a=tf.constant(5.0)\n",
    "    b=tf.constant(5.0)\n",
    "    \n",
    "    # Multiply the constants with each other\n",
    "    c=tf.multiply(a,b)\n",
    "    \n",
    "\n",
    "# Create a session to execute the dataflow graph\n",
    "with tf.Session(graph=main_graph) as session:\n",
    "    \n",
    "    # Perform the calculation defined in the dataflow graph and get the result\n",
    "    output=session.run(c)\n",
    "    print('Result of the multiplication: %d '%output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How you will feed python external data ? in simple terms (x_train and x_test) how you will feed that data in dataflow graphs for that we need placeholders \n",
    "#### so with help of placeholders you can define graphs earlier and can feed the data later on that is possible because with the help of placeholders\n",
    "#### so we define empty placeholder which will take data later training time and feed this data in data flow graphs\n",
    "\n",
    "#### Q3 When python a array becomes a tensor after feeding in neural network or when the values  flows in dataflow graphs ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now to feed data in place holder we just need a dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5. 14. 12. 72.  8.]\n"
     ]
    }
   ],
   "source": [
    "# Create an empty graph, that will be filled with operation nodes later\n",
    "main_graph=tf.Graph()\n",
    "\n",
    "# Register this graph as default graph. \n",
    "# All operations within this context will become operation nodes in this graph\n",
    "with main_graph.as_default():\n",
    "    \n",
    "    # Define the placeholders that will feed python arrays into the dataflow graph\n",
    "    a=tf.placeholder(name='a', shape=[5], dtype=tf.float32)\n",
    "    b=tf.placeholder(name='b', shape=[5], dtype=tf.float32)\n",
    "    \n",
    "    c=tf.multiply(a,b)\n",
    "    \n",
    "# Create a session to execute the dataflow graph\n",
    "with tf.Session(graph=main_graph) as session:\n",
    "    \n",
    "    # Perform the calculation defined in the dataflow graph and get the result.\n",
    "    # We must provide the values for the placeholders with \"feed_dict\" dictionary\n",
    "    output=session.run(c, feed_dict={a: [5.0,7.0,3.0,9.0,2.0],\n",
    "                                     b: [1.0,2.0,4.0,8.0,4.0],\n",
    "                                     })\n",
    "    print(output)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carrying out Forward Propagation (TF variables)\n",
    "* for defining weight matrix and biases we need another concept to learn called as tensorflow variables \n",
    "\n",
    "** Note placeholder and constant can also define tensor same as tf variables \n",
    "but why need it ?\n",
    "\n",
    "see till now we haven't considered any trainable paramters which are getting dynamically changes up after every iteration , with help of constants by definition it is constant and place-holder to feed data which again a mapped dict so now need different tensor whose values is always a variable (dynamic change requires) from their concept came up of tensor variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now here are 2 things first we have tf.Variable() and another is .get_variable() the first one takes up intial values and second one take random values from a probability distribution of our choice for training weights second method is considered more rather than first because we want our weights to be random during initalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how to initialize any varibale (how to send them) in the dataflow graph\n",
    "### so when dataflow graph is executed in session variables are initialized but for that any variable in data flow graph you wanna intialize is done with tf.global_variables_intializer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Result of the forward propagation: \n",
      "\n",
      "[[ 0.7252588   0.99996537 -0.97938305]]\n"
     ]
    }
   ],
   "source": [
    "# Import numpy for some array reshaping\n",
    "import numpy as np\n",
    "\n",
    "# Create an empty graph, that will be filled with operation nodes later\n",
    "main_graph=tf.Graph()\n",
    "\n",
    "# Register this graph as default graph. \n",
    "# All operations within this context will become operation nodes in this graph\n",
    "with main_graph.as_default():\n",
    "    # Placeholder for data input\n",
    "    input_=tf.placeholder(dtype=tf.float32, shape=[1,5], name='input')\n",
    "    \n",
    "    ###Define the weight matrices###\n",
    "    # Weight matrix, that connects the input and the 1st hidden layer\n",
    "    W1=tf.get_variable(name='W1', shape=[5,10], initializer=tf.random_normal_initializer)\n",
    "    # Weight matrix, that connects the 1st hidden layer and the 2nd hidden layer\n",
    "    W2=tf.get_variable(name='W2', shape=[10,10], initializer=tf.random_normal_initializer)\n",
    "    # Weight matrix, that connects the 2nd hidden layer and the 3rd hidden layer\n",
    "    W3=tf.get_variable(name='W3', shape=[10,10], initializer=tf.random_normal_initializer)\n",
    "    # Weight matrix, that connects the 3rd hidden layer and the output layer\n",
    "    W4=tf.get_variable(name='W4', shape=[10,3], initializer=tf.random_normal_initializer)\n",
    "         \n",
    "    ####Define the forward propagation operations###\n",
    "      \n",
    "    #1st hidden layer\n",
    "    z1=tf.matmul(input_, W1)\n",
    "    a1=tf.nn.tanh(z1)\n",
    "    \n",
    "    #2nd hidden layer\n",
    "    z2=tf.matmul(a1, W2)\n",
    "    a2=tf.nn.tanh(z2)\n",
    "    \n",
    "    #3rd hidden layer\n",
    "    z3=tf.matmul(a2, W3)\n",
    "    a3=tf.nn.tanh(z3)\n",
    "    \n",
    "    #output layer\n",
    "    z_out=tf.matmul(a3, W4)\n",
    "    output=tf.nn.tanh(z_out)\n",
    "\n",
    "\n",
    "# Create a session to execute the dataflow graph   \n",
    "with tf.Session(graph=main_graph) as sess:\n",
    "    \n",
    "    # Initialize the weight matrix\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Define some random input\n",
    "    x=np.array([1.0, 2.5, 0.7, 3.0, 9.0]).reshape([1,5])\n",
    "    \n",
    "    # Start the forward propagation step\n",
    "    prediction=sess.run(output, feed_dict={input_: x})\n",
    "    \n",
    "    print('\\n\\n\\nResult of the forward propagation: \\n')\n",
    "    print(prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### people familiar with forward propagation can understand this code\n",
    "\n",
    "### to make it as you can see that (sess) will initialize everything above it \n",
    "\n",
    "### to initialze anything we need to use sess.run() (* not to forget point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving on 2nd part of this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So here tutorial is all about components which we need to understand how to call them and setup with feed forward propagation (NN)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mse use case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Output of the neural network: \n",
      "\n",
      "[[ 0.8474757 -1.       ]]\n",
      "\n",
      "Mean squared error loss: 13.12\n"
     ]
    }
   ],
   "source": [
    "# Import numpy for some array reshaping\n",
    "import numpy as np\n",
    "\n",
    "# Create an empty graph, that will be filled with operation nodes later\n",
    "main_graph=tf.Graph()\n",
    "\n",
    "# Register this graph as default graph. \n",
    "# All operations within this context will become operation nodes in this graph\n",
    "with main_graph.as_default():\n",
    "    \n",
    "    # Placeholder for the input data\n",
    "    input_= tf.placeholder(dtype=tf.float32, shape=[1,3], name='input')\n",
    "    # Placeholder for the label\n",
    "    label = tf.placeholder(dtype=tf.float32, shape=[1,2], name='label')\n",
    "    \n",
    "    # Define a 3x2 weight matrix\n",
    "    W=tf.get_variable(name='weights', shape=[3,2], dtype=tf.float32)\n",
    "    \n",
    "    # Compute a forward propagation step\n",
    "    output=tf.nn.tanh(tf.matmul(input_, W))\n",
    "    \n",
    "    # Use build-in function for the mean squared error loss\n",
    "    loss_op=tf.losses.mean_squared_error(labels=label, predictions=output)\n",
    "    \n",
    "\n",
    "# Create a session to execute the dataflow graph\n",
    "\n",
    "with tf.Session(graph=main_graph) as sess:\n",
    "    \n",
    "    # Initialize the weight matrix\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Define some random input (x) and a label (y)\n",
    "    # Reshape the arrays into the shape of the placeholders\n",
    "    x=np.reshape([10.0,3.0,4.0], [1,3])\n",
    "    y=np.reshape([5.0,2.0], [1,2])\n",
    "\n",
    "    # Compute the prediction of the network\n",
    "    nn_output=sess.run(output, feed_dict={input_: x})\n",
    "\n",
    "    # Compute the mean squared error loss\n",
    "    mse_loss=sess.run(loss_op, feed_dict={input_: x,\n",
    "                                         label: y})\n",
    "    print('\\n\\n\\nOutput of the neural network: \\n')\n",
    "    print(nn_output)\n",
    "    \n",
    "    print('\\nMean squared error loss: %.2f'%mse_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross entropy and sparse cross entropy example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Cross entropy loss: 0.04\n",
      "Sparse cross entropy loss: 0.04\n"
     ]
    }
   ],
   "source": [
    "# Import numpy for some array reshaping\n",
    "import numpy as np\n",
    "\n",
    "# Create an empty graph, that will be filled with operation nodes later\n",
    "main_graph=tf.Graph()\n",
    "\n",
    "# Register this graph as default graph. \n",
    "# All operations within this context will become operation nodes in this graph\n",
    "with main_graph.as_default():\n",
    "    \n",
    "    # Placeholder for the input data\n",
    "    input_= tf.placeholder(dtype=tf.float32, shape=[1,3], name='input')\n",
    "    # Placeholder for the scalar label\n",
    "    label = tf.placeholder(dtype=tf.int32, shape=[1], name='label_sparse')\n",
    "    # Placeholder for the one-hot-encoded label\n",
    "    label_ohe = tf.placeholder(dtype=tf.float32, shape=[1,5], name='label_one_hot_encoded')\n",
    "\n",
    "    # Define a 3x5 weight matrix\n",
    "    W=tf.get_variable(name='weights', shape=[3,5], dtype=tf.float32)\n",
    "    \n",
    "    # Forward propagation without an activation function\n",
    "    logits=tf.matmul(input_, W)\n",
    "    \n",
    "    # Cross entropy loss operation, that requires the one-hot-encoded label\n",
    "    loss_op=tf.nn.softmax_cross_entropy_with_logits_v2(labels=label_ohe, \n",
    "                                                       logits=logits, \n",
    "                                                       name='cross_entropy_loss')\n",
    "\n",
    "    # Cross entropy loss operation, that requires the scalar label\n",
    "    loss_op_sparse=tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, \n",
    "                                                                  logits=logits, \n",
    "                                                                  name='sparse_cross_entropy_loss')\n",
    "\n",
    "\n",
    "# Create a session to execute the dataflow graph\n",
    "with tf.Session(graph=main_graph) as sess:\n",
    "    \n",
    "    # Initialize the weight matrix\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Define some random input (x)\n",
    "    x=np.reshape([10.0,3.0,4.0], [1,3])\n",
    "    \n",
    "    # Define the scalar label \n",
    "    y=np.reshape([2], [1])\n",
    "    # The same label but one-hot-encoded version of it\n",
    "    y_ohe=np.reshape([0,0,1,0,0], [1,5])\n",
    "    \n",
    "    # Run the one cross entropy loss operation\n",
    "    loss=sess.run(loss_op, feed_dict={input_:x, \n",
    "                                      label_ohe: y_ohe\n",
    "                                      })\n",
    "    \n",
    "    # Run the other cross entropy loss operation\n",
    "    sparse_loss=sess.run(loss_op_sparse, feed_dict={input_:x,\n",
    "                                                    label: y\n",
    "                                                    })\n",
    "    \n",
    "    print('\\n\\n\\nCross entropy loss: %.2f'%loss)\n",
    "    print('Sparse cross entropy loss: %.2f'%sparse_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy for some array reshaping\n",
    "import numpy as np\n",
    "\n",
    "# Create an empty graph, that will be filled with operation nodes later\n",
    "main_graph=tf.Graph()\n",
    "\n",
    "# Register this graph as default graph. \n",
    "# All operations within this context will become operation nodes in this graph\n",
    "with main_graph.as_default():\n",
    "    \n",
    "    # Placeholder for data input\n",
    "    input_=tf.placeholder(dtype=tf.float32, shape=[1,5], name='input')\n",
    "    # Placeholder for the label\n",
    "    labels=tf.placeholder(dtype=tf.float32, shape=[1,1], name='labels')\n",
    "\n",
    "    # Define a 5x1 weight matrix\n",
    "    W=tf.get_variable(name='weights', shape=[5,1])\n",
    "    \n",
    "    # Forward propagation\n",
    "    forward=tf.nn.tanh(tf.matmul(input_, W))\n",
    "\n",
    "    # Mean squared error loss function\n",
    "    loss=tf.losses.mean_squared_error(labels, forward)\n",
    "    \n",
    "    # Define the instance of the class that performs the gradient descent step\n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "    \n",
    "    # Get all trainable parameters of the network (here: the weight matrix W)\n",
    "    trainable_variables=tf.trainable_variables()\n",
    "    \n",
    "    # Compute the gradients of the loss function with respect to the weights\n",
    "    gradients= tf.gradients(loss, trainable_variables)\n",
    "    \n",
    "    # Perform the gradient descent step.\n",
    "    # The input argument are tuples. Each tuple is a pair of the gradient and the weight \n",
    "    # that was used to calculate this gradient\n",
    "    update_step=optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "     \n",
    "    \n",
    "    #Alternative Solution \n",
    "    update_step_alternative=tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n",
    "       \n",
    "    \n",
    "    \n",
    "# Create a session to execute the dataflow graph   \n",
    "with tf.Session(graph=main_graph) as sess:\n",
    "    \n",
    "    # Initialize the weight matrix\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Define some random input and a label\n",
    "    x=np.array([1,1,0,2,5]).reshape([1,5])\n",
    "    y=np.array([2]).reshape([1,1]) \n",
    "    \n",
    "    # Perform one single gradient descent step\n",
    "    sess.run(update_step, feed_dict={input_: x,\n",
    "                                     labels: y\n",
    "                                     }) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Neural network regression must see example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generation of Data...\n",
      "\n",
      "binary number: 00000000001, decimal number: 1\n",
      "binary number: 00000000010, decimal number: 2\n",
      "binary number: 00000000011, decimal number: 3\n",
      "binary number: 00000000100, decimal number: 4\n",
      "binary number: 00000000101, decimal number: 5\n",
      "binary number: 00000000110, decimal number: 6\n",
      "binary number: 00000000111, decimal number: 7\n",
      "binary number: 00000001000, decimal number: 8\n",
      "binary number: 00000001001, decimal number: 9\n",
      "binary number: 00000001010, decimal number: 10\n",
      "binary number: 11111110110, decimal number: 2038\n",
      "binary number: 11111110111, decimal number: 2039\n",
      "binary number: 11111111000, decimal number: 2040\n",
      "binary number: 11111111001, decimal number: 2041\n",
      "binary number: 11111111010, decimal number: 2042\n",
      "binary number: 11111111011, decimal number: 2043\n",
      "binary number: 11111111100, decimal number: 2044\n",
      "binary number: 11111111101, decimal number: 2045\n",
      "binary number: 11111111110, decimal number: 2046\n",
      "binary number: 11111111111, decimal number: 2047\n",
      "[[1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1.]\n",
      " [0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1.]\n",
      " [1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1.]\n",
      " [1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1.]\n",
      " [0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1.]\n",
      " [1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1.]\n",
      " [0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0.]]\n",
      "[[0.5300439667806546]\n",
      " [0.3170493404982902]\n",
      " [0.9540791402051783]\n",
      " [0.23888617489008304]\n",
      " [0.13629702002931118]\n",
      " [0.5168539325842697]\n",
      " [0.032730825598436736]\n",
      " [0.36541279921836833]\n",
      " [0.29555446995603324]\n",
      " [0.22618466047874938]\n",
      " [0.7938446507083536]\n",
      " [0.82266731802638]\n",
      " [0.32046897899364923]\n",
      " [0.18124084025403028]\n",
      " [0.4167073766487543]\n",
      " [0.25598436736687835]]\n",
      "\n",
      "\n",
      "Start of the training...\n",
      "\n",
      "epoch_nr.: 0, loss: 0.057\n",
      "epoch_nr.: 1, loss: 0.014\n",
      "epoch_nr.: 2, loss: 0.011\n",
      "epoch_nr.: 3, loss: 0.010\n",
      "epoch_nr.: 4, loss: 0.008\n",
      "epoch_nr.: 5, loss: 0.008\n",
      "epoch_nr.: 6, loss: 0.007\n",
      "epoch_nr.: 7, loss: 0.006\n",
      "epoch_nr.: 8, loss: 0.006\n",
      "epoch_nr.: 9, loss: 0.005\n",
      "Binary number: [1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.],  label: 1096, prediciton 1138\n",
      "Binary number: [0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1.],  label: 241, prediciton 0\n",
      "Binary number: [1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0.],  label: 1694, prediciton 1726\n",
      "Binary number: [1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0.],  label: 1322, prediciton 1620\n",
      "Binary number: [0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0.],  label: 356, prediciton 658\n",
      "Binary number: [1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1.],  label: 1173, prediciton 1094\n",
      "Binary number: [1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1.],  label: 1843, prediciton 2009\n",
      "Binary number: [0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1.],  label: 949, prediciton 992\n",
      "Binary number: [1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0.],  label: 1362, prediciton 1488\n",
      "Binary number: [1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1.],  label: 1887, prediciton 1874\n",
      "Binary number: [1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0.],  label: 1512, prediciton 1511\n",
      "Binary number: [0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1.],  label: 971, prediciton 873\n",
      "Binary number: [1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.],  label: 1080, prediciton 1010\n",
      "Binary number: [1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1.],  label: 1781, prediciton 1709\n",
      "Binary number: [0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0.],  label: 856, prediciton 574\n",
      "Binary number: [1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0.],  label: 2006, prediciton 1877\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "# list that will contain our training set\n",
    "training_data=[]\n",
    "\n",
    "# How many digits should this binary number have?\n",
    "n=11\n",
    "    \n",
    "#Mini-batch size\n",
    "batch_size=16\n",
    "\n",
    "print('\\n\\nGeneration of Data...\\n') \n",
    "for i in np.arange(2048):\n",
    "    \n",
    "    # Create a binary number of type string\n",
    "    b = bin(i)[2:]\n",
    "    l = len(b)\n",
    "    b = str(0) * (n - l) + b  \n",
    "\n",
    "    # Convert the binary string number to type float\n",
    "    features=np.array(list(b)).astype(float)\n",
    "    \n",
    "    # Create and normalize the corresponding decimal label\n",
    "    label=float(i)/2047\n",
    "    \n",
    "    # Put the feature-label instance into the list\n",
    "    training_data.append([features, label])\n",
    "    \n",
    "    if (i>=1 and i<11) or (i>=2038):\n",
    "        print('binary number: %s, decimal number: %d' %(b, i))\n",
    "        \n",
    "# shuffle the data\n",
    "shuffle(training_data)  \n",
    "\n",
    "# convert the list to np.array     \n",
    "training_data=np.array(training_data)\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "# Get the next mini-batch of training samples\n",
    "def get_next_batch(n_batch):\n",
    "    \n",
    "    # Get the next mini-batch of training samples from the dataset\n",
    "    features=training_data[n_batch*batch_size:(batch_size*(1+n_batch)),0]\n",
    "    labels=training_data[n_batch*batch_size:(batch_size*(1+n_batch)),1]\n",
    "    \n",
    "    # Reshape the list of arrays into a nxn np.array\n",
    "    features = np.concatenate(features).reshape([batch_size,11])  \n",
    "    # Reshape the labels \n",
    "    labels=np.reshape(labels, [batch_size,1])\n",
    "    \n",
    "    return features, labels\n",
    "    \n",
    "features, labels=get_next_batch(n_batch=1)\n",
    "\n",
    "print(features)\n",
    "print(labels)\n",
    "#%%  \n",
    "\n",
    "# Create the training graph\n",
    "main_graph=tf.Graph()\n",
    "\n",
    "with main_graph.as_default():\n",
    "    \n",
    "    # Define the placeholders for the features and the labels\n",
    "    x=tf.placeholder(dtype=tf.float32,shape=[batch_size,11], name='features')\n",
    "    y=tf.placeholder(dtype=tf.float32, shape=[batch_size,1], name='labels')\n",
    "           \n",
    "    # Create the weight matrices and the bias vectors \n",
    "    initializer=initializer=tf.random_normal_initializer(mean=0.0, stddev=0.25)\n",
    "   \n",
    "    W1=tf.get_variable('W1',shape=[11,50], initializer=initializer)\n",
    "    W2=tf.get_variable('W2',shape=[50,25], initializer=initializer)\n",
    "    W3=tf.get_variable('W3',shape=[25,1], initializer=initializer)\n",
    "    \n",
    "    b1=tf.get_variable('b1',shape=[50], initializer=initializer)\n",
    "    b2=tf.get_variable('b2',shape=[25], initializer=initializer)\n",
    "\n",
    "    ### Define the forward propagation step ###\n",
    "    \n",
    "    # First hidden layer\n",
    "    z1=tf.matmul(x,W1)+b1\n",
    "    a1=tf.nn.tanh(z1)\n",
    "    \n",
    "    # Second hidden layer\n",
    "    z2=tf.matmul(a1,W2)+b2\n",
    "    a2=tf.nn.tanh(z2)\n",
    "    \n",
    "    # Outputlayer\n",
    "    predict_op=tf.nn.relu(tf.matmul(a2,W3))\n",
    "    \n",
    "    # Define the loss function\n",
    "    loss_op=tf.losses.mean_squared_error(y,predict_op)\n",
    "       \n",
    "    # Perform a gradient descent step\n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    trainable_parameters = tf.trainable_variables()\n",
    "    gradients = tf.gradients(loss_op,trainable_parameters)\n",
    "    update_step = optimizer.apply_gradients(zip(gradients, trainable_parameters))\n",
    "\n",
    "\n",
    "print('\\n\\nStart of the training...\\n')\n",
    "with tf.Session(graph=main_graph) as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # How many mini-batches in total?\n",
    "    num_batches=int(2048/batch_size)\n",
    "    \n",
    "    loss=0\n",
    "\n",
    "    #Iterate over the entire training set for 10 times\n",
    "    for epoch in range(10):\n",
    "            \n",
    "        # Iterate over the number of mini-batches\n",
    "        for n_batch in range(num_batches-1):\n",
    "            \n",
    "            # Get the next mini-batches of samples for the training set\n",
    "            features, labels=get_next_batch(n_batch)\n",
    "              \n",
    "            # Perform the gradient descent step on that mini-batch and compute the loss value\n",
    "            _, loss_=sess.run((update_step, loss_op), feed_dict={x:features, y:labels})   \n",
    "            loss+=loss_\n",
    "             \n",
    "        print('epoch_nr.: %i, loss: %.3f' %(epoch,(loss/num_batches)))\n",
    "        loss=0 \n",
    "\n",
    "    # Compute the prediction on the last mini-batch\n",
    "    prediction=sess.run(predict_op, feed_dict={x:features})\n",
    "    \n",
    "    # Iterate over the features and labels from the last mini-batch as well as\n",
    "    # the predicitons made by the network, and compare them to check the performance\n",
    "    for f, l, p in zip(features, labels, prediction):\n",
    "        \n",
    "        # Rescale the predictions and labels back into their original value range\n",
    "        p=p*2047\n",
    "        l=l*2047\n",
    "        \n",
    "        print('Binary number: %s,  label: %i, prediciton %i' %(str(f), l,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Neural network for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generation of Data...\n",
      "\n",
      "binary number: 0000, decimal number: 0\n",
      "binary number: 0001, decimal number: 1\n",
      "binary number: 0010, decimal number: 2\n",
      "binary number: 0011, decimal number: 3\n",
      "binary number: 0100, decimal number: 4\n",
      "binary number: 0101, decimal number: 5\n",
      "binary number: 0110, decimal number: 6\n",
      "binary number: 0111, decimal number: 7\n",
      "binary number: 1000, decimal number: 8\n",
      "binary number: 1001, decimal number: 9\n",
      "\n",
      "\n",
      "Mini-batch of features: \n",
      "\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 1.]\n",
      " [0. 1. 1. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "\n",
      "Mini-batch of labels: \n",
      "\n",
      "[8 4 9 7 2 6 1 0]\n",
      "\n",
      "\n",
      "Start of the training...\n",
      "\n",
      "epoch_nr.: 0, loss: 1.987\n",
      "epoch_nr.: 1, loss: 1.423\n",
      "epoch_nr.: 2, loss: 1.012\n",
      "epoch_nr.: 3, loss: 0.724\n",
      "epoch_nr.: 4, loss: 0.531\n",
      "epoch_nr.: 5, loss: 0.403\n",
      "epoch_nr.: 6, loss: 0.316\n",
      "epoch_nr.: 7, loss: 0.255\n",
      "epoch_nr.: 8, loss: 0.211\n",
      "epoch_nr.: 9, loss: 0.179\n",
      "\n",
      "\n",
      "Testing the neural network:\n",
      "\n",
      "Binary number: [0. 0. 0. 1.], decimal number: 1, predicted_class: 1, predicted_prob_score: 0.845\n",
      "Binary number: [0. 0. 0. 0.], decimal number: 0, predicted_class: 0, predicted_prob_score: 0.847\n",
      "Binary number: [0. 0. 1. 1.], decimal number: 3, predicted_class: 3, predicted_prob_score: 0.842\n",
      "Binary number: [0. 1. 0. 1.], decimal number: 5, predicted_class: 5, predicted_prob_score: 0.843\n",
      "Binary number: [1. 0. 0. 0.], decimal number: 8, predicted_class: 8, predicted_prob_score: 0.874\n",
      "Binary number: [0. 1. 0. 0.], decimal number: 4, predicted_class: 4, predicted_prob_score: 0.853\n",
      "Binary number: [1. 0. 0. 1.], decimal number: 9, predicted_class: 9, predicted_prob_score: 0.844\n",
      "Binary number: [0. 1. 1. 1.], decimal number: 7, predicted_class: 7, predicted_prob_score: 0.841\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "# list that will contain our training set\n",
    "training_data=[]\n",
    "\n",
    "# How many digits should this binary number have?\n",
    "n=4\n",
    "    \n",
    "#Mini-batch size\n",
    "batch_size=8\n",
    "\n",
    "print('\\n\\nGeneration of Data...\\n') \n",
    "for i in np.arange(0, 10):\n",
    "    \n",
    "    # Create a binary number of type string\n",
    "    b = bin(i)[2:]\n",
    "    l = len(b)\n",
    "    b = str(0) * (n - l) + b  \n",
    "\n",
    "    # Convert binary string number to type float\n",
    "    features=np.array(list(b)).astype(float)\n",
    "    # Create the corresponding binary label / class\n",
    "    label=i\n",
    "    \n",
    "    # Put the feature-label pair into the list\n",
    "    training_data.append([features, label])\n",
    "\n",
    "    print('binary number: %s, decimal number: %d' %(b, i))\n",
    "        \n",
    "    \n",
    "#%%\n",
    "# shuffle the data\n",
    "shuffle(training_data)  \n",
    "\n",
    "training_data=training_data*1000\n",
    "\n",
    "# convert the list to np.array     \n",
    "training_data=np.array(training_data)\n",
    "\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "# Get the next mini-batch of training samples\n",
    "def get_next_batch(n_batch):\n",
    "    \n",
    "    # Get the next mini-batch of training samples from the dataset\n",
    "    features=training_data[n_batch*batch_size:(batch_size*(1+n_batch)),0]\n",
    "    labels=training_data[n_batch*batch_size:(batch_size*(1+n_batch)),1]\n",
    "    \n",
    "    # Reshape the list of arrays into a nxn np.array\n",
    "    features = np.concatenate(features).reshape([batch_size, 4])  \n",
    "    # Reshape the labels \n",
    "    labels=np.reshape(labels, [batch_size])\n",
    "    \n",
    "    return features, labels\n",
    "    \n",
    "features, labels=get_next_batch(n_batch=1)\n",
    "\n",
    "print('\\n\\nMini-batch of features: \\n')\n",
    "print(features)\n",
    "print('\\n\\nMini-batch of labels: \\n')\n",
    "print(labels)\n",
    "\n",
    "#%%  \n",
    "\n",
    "# Create the training graph\n",
    "main_graph=tf.Graph()\n",
    "\n",
    "with main_graph.as_default():\n",
    "    \n",
    "    # Define the placeholders for the features and the labels\n",
    "    x=tf.placeholder(dtype=tf.float32,shape=[batch_size, 4], name='features')\n",
    "    y=tf.placeholder(dtype=tf.int32, shape=[batch_size], name='labels')\n",
    "           \n",
    "    # Create the weight matrices and the bias vectors \n",
    "    initializer=initializer=tf.random_normal_initializer(mean=0.0, stddev=0.25)\n",
    "   \n",
    "    W1=tf.get_variable('W1',shape=[4,50], initializer=initializer)\n",
    "    W2=tf.get_variable('W2',shape=[50,25], initializer=initializer)\n",
    "    W3=tf.get_variable('W3',shape=[25,10], initializer=initializer)\n",
    "    \n",
    "    b1=tf.get_variable('b1',shape=[50], initializer=initializer)\n",
    "    b2=tf.get_variable('b2',shape=[25], initializer=initializer)\n",
    "\n",
    "    ### Define the forward propagation step ###\n",
    "    \n",
    "    # First hidden layer\n",
    "    z1=tf.matmul(x,W1)+b1\n",
    "    a1=tf.nn.tanh(z1)\n",
    "    \n",
    "    # Second hidden layer\n",
    "    z2=tf.matmul(a1,W2)+b2\n",
    "    a2=tf.nn.tanh(z2)\n",
    "    \n",
    "    # Outputlayer, without an activation function (input for the loss function)\n",
    "    logits=tf.matmul(a2,W3)\n",
    "       \n",
    "    # Compute the probability scores after the training)\n",
    "    probs=tf.nn.softmax(logits)\n",
    "    \n",
    "    # Define the loss function\n",
    "    loss_op=tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)\n",
    "       \n",
    "    # Perform a gradient descent step\n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    trainable_parameters = tf.trainable_variables()\n",
    "    gradients = tf.gradients(loss_op,trainable_parameters)\n",
    "    update_step = optimizer.apply_gradients(zip(gradients, trainable_parameters))\n",
    "\n",
    "\n",
    "print('\\n\\nStart of the training...\\n')\n",
    "with tf.Session(graph=main_graph) as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # How many mini-batches in total?\n",
    "    num_batches=int(10000/batch_size)\n",
    "    \n",
    "    loss=0\n",
    "\n",
    "    #Iterate over the entire training set for 10 times\n",
    "    for epoch in range(10):\n",
    "            \n",
    "        # Iterate over the number of mini-batches\n",
    "        for n_batch in range(num_batches-1):\n",
    "            \n",
    "            # Get the next mini-batches of samples for the training set\n",
    "            features, labels=get_next_batch(n_batch)\n",
    "              \n",
    "            # Perform the gradient descent step on that mini-batch and compute the loss value\n",
    "            _, loss_=sess.run((update_step, loss_op), feed_dict={x:features, y:labels})   \n",
    "            \n",
    "            loss+=loss_\n",
    "             \n",
    "        print('epoch_nr.: %i, loss: %.3f' %(epoch,(loss/num_batches)))\n",
    "        loss=0 \n",
    "    \n",
    "    \n",
    "    print('\\n\\nTesting the neural network:\\n')\n",
    "    # Compute the probability scores for the last mini-batch\n",
    "    prob_scores=sess.run(probs, feed_dict={x:features, y:labels})\n",
    "    \n",
    "    # Iterate over the features and labels from the last mini-batch as well as\n",
    "    # the predicitons made by the network, and compare them to check the performance\n",
    "    for f, l, p in zip(features, labels, prob_scores):\n",
    "    \n",
    "        # Get the class with the highest probability score\n",
    "        predicted_class=np.argmax(p)\n",
    "        # Get the actual probability score\n",
    "        predicted_class_score=np.max(p)\n",
    "     \n",
    "        print('Binary number: %s, decimal number: %i, predicted_class: %i, predicted_prob_score: %.3f' \n",
    "              %(str(f), l, predicted_class, predicted_class_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generation of Data...\n",
      "\n",
      "binary number: 00000000001, decimal number: 1\n",
      "binary number: 00000000010, decimal number: 2\n",
      "binary number: 00000000011, decimal number: 3\n",
      "binary number: 00000000100, decimal number: 4\n",
      "binary number: 00000000101, decimal number: 5\n",
      "binary number: 00000000110, decimal number: 6\n",
      "binary number: 00000000111, decimal number: 7\n",
      "binary number: 00000001000, decimal number: 8\n",
      "binary number: 00000001001, decimal number: 9\n",
      "binary number: 00000001010, decimal number: 10\n",
      "binary number: 11111110110, decimal number: 2038\n",
      "binary number: 11111110111, decimal number: 2039\n",
      "binary number: 11111111000, decimal number: 2040\n",
      "binary number: 11111111001, decimal number: 2041\n",
      "binary number: 11111111010, decimal number: 2042\n",
      "binary number: 11111111011, decimal number: 2043\n",
      "binary number: 11111111100, decimal number: 2044\n",
      "binary number: 11111111101, decimal number: 2045\n",
      "binary number: 11111111110, decimal number: 2046\n",
      "binary number: 11111111111, decimal number: 2047\n",
      "[[0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1.]\n",
      " [0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0.]\n",
      " [1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1.]\n",
      " [0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1.]\n",
      " [1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0.]\n",
      " [0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0.]\n",
      " [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      " [0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0.]]\n",
      "[[0.055202735710796286]\n",
      " [0.02051783097215437]\n",
      " [0.5564240351734245]\n",
      " [0.1787982413287738]\n",
      " [0.7772349780166097]\n",
      " [0.48949682462139715]\n",
      " [0.20468978993649242]\n",
      " [0.6819736199316072]\n",
      " [0.49145090376160233]\n",
      " [0.6209086468001954]\n",
      " [0.9721543722520762]\n",
      " [0.0004885197850512946]\n",
      " [0.268685881778212]\n",
      " [0.3869076697606253]\n",
      " [0.11284807034684904]\n",
      " [0.4582315583781143]]\n",
      "\n",
      "\n",
      "Start of the training...\n",
      "\n",
      "epoch_nr.: 0, loss: 0.271\n",
      "epoch_nr.: 1, loss: 0.196\n",
      "epoch_nr.: 2, loss: 0.145\n",
      "epoch_nr.: 3, loss: 0.099\n",
      "epoch_nr.: 4, loss: 0.076\n",
      "epoch_nr.: 5, loss: 0.059\n",
      "epoch_nr.: 6, loss: 0.041\n",
      "epoch_nr.: 7, loss: 0.037\n",
      "epoch_nr.: 8, loss: 0.035\n",
      "epoch_nr.: 9, loss: 0.032\n",
      "Binary number: [0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0.],  label: 74, prediciton 910\n",
      "Binary number: [0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1.],  label: 147, prediciton 286\n",
      "Binary number: [0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0.],  label: 30, prediciton 0\n",
      "Binary number: [0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1.],  label: 787, prediciton 926\n",
      "Binary number: [1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.],  label: 1029, prediciton 1439\n",
      "Binary number: [0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0.],  label: 484, prediciton 1238\n",
      "Binary number: [1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.],  label: 1045, prediciton 742\n",
      "Binary number: [0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1.],  label: 47, prediciton 210\n",
      "Binary number: [1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1.],  label: 1267, prediciton 1168\n",
      "Binary number: [1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1.],  label: 1873, prediciton 1806\n",
      "Binary number: [1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.],  label: 1106, prediciton 714\n",
      "Binary number: [0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.],  label: 127, prediciton 0\n",
      "Binary number: [1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0.],  label: 1858, prediciton 1552\n",
      "Binary number: [0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0.],  label: 280, prediciton 509\n",
      "Binary number: [0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0.],  label: 172, prediciton 245\n",
      "Binary number: [0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1.],  label: 983, prediciton 1352\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "# list that will contain our training set\n",
    "training_data=[]\n",
    "\n",
    "# How many digits should this binary number have?\n",
    "n=11\n",
    "    \n",
    "#Mini-batch size\n",
    "batch_size=16\n",
    "\n",
    "print('\\n\\nGeneration of Data...\\n') \n",
    "for i in np.arange(2048):\n",
    "    \n",
    "    # Create a binary number of type string\n",
    "    b = bin(i)[2:]\n",
    "    l = len(b)\n",
    "    b = str(0) * (n - l) + b  \n",
    "\n",
    "    # Convert the binary string number to type float\n",
    "    features=np.array(list(b)).astype(float)\n",
    "    \n",
    "    # Create and normalize the corresponding decimal label\n",
    "    label=float(i)/2047\n",
    "    \n",
    "    # Put the feature-label instance into the list\n",
    "    training_data.append([features, label])\n",
    "    \n",
    "    if (i>=1 and i<11) or (i>=2038):\n",
    "        print('binary number: %s, decimal number: %d' %(b, i))\n",
    "        \n",
    "# shuffle the data\n",
    "shuffle(training_data)  \n",
    "\n",
    "# convert the list to np.array     \n",
    "training_data=np.array(training_data)\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "# Get the next mini-batch of training samples\n",
    "def get_next_batch(n_batch):\n",
    "    \n",
    "    # Get the next mini-batch of training samples from the dataset\n",
    "    features=training_data[n_batch*batch_size:(batch_size*(1+n_batch)),0]\n",
    "    labels=training_data[n_batch*batch_size:(batch_size*(1+n_batch)),1]\n",
    "    \n",
    "    # Reshape the list of arrays into a nxn np.array\n",
    "    features = np.concatenate(features).reshape([batch_size,11])  \n",
    "    # Reshape the labels \n",
    "    labels=np.reshape(labels, [batch_size,1])\n",
    "    \n",
    "    return features, labels\n",
    "    \n",
    "features, labels=get_next_batch(n_batch=1)\n",
    "\n",
    "print(features)\n",
    "print(labels)\n",
    "#%%  \n",
    "\n",
    "# Create the training graph\n",
    "main_graph=tf.Graph()\n",
    "\n",
    "with main_graph.as_default():\n",
    "    \n",
    "    # Define the placeholders for the features and the labels\n",
    "    x=tf.placeholder(dtype=tf.float32,shape=[batch_size,11], name='features')\n",
    "    y=tf.placeholder(dtype=tf.float32, shape=[batch_size,1], name='labels')\n",
    "           \n",
    "    # Create the weight matrices and the bias vectors \n",
    "    initializer=initializer=tf.random_normal_initializer(mean=0.0, stddev=0.25)\n",
    "   \n",
    "    W1=tf.get_variable('W1',shape=[11,50], initializer=initializer)\n",
    "    W2=tf.get_variable('W2',shape=[50,25], initializer=initializer)\n",
    "    W3=tf.get_variable('W3',shape=[25,1], initializer=initializer)\n",
    "    \n",
    "    b1=tf.get_variable('b1',shape=[50], initializer=initializer)\n",
    "    b2=tf.get_variable('b2',shape=[25], initializer=initializer)\n",
    "\n",
    "    ### Define the forward propagation step with Dropout ###\n",
    "    \n",
    "    z1=tf.matmul(x,W1)+b1\n",
    "    a1=tf.nn.tanh(z1)\n",
    "    a1_dropout= tf.nn.dropout(a1, rate=0.25)\n",
    "    \n",
    "    z2=tf.matmul(a1_dropout,W2)+b2\n",
    "    a2=tf.nn.tanh(z2)\n",
    "    a2_dropout= tf.nn.dropout(a2, rate=0.25)\n",
    "    \n",
    "    # Outputlayer\n",
    "    predict_op=tf.nn.relu(tf.matmul(a2_dropout,W3))\n",
    "    \n",
    "    # Define the loss function\n",
    "    loss_op=tf.losses.mean_squared_error(y,predict_op)\n",
    "       \n",
    "    # Perform a gradient descent step\n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    trainable_parameters = tf.trainable_variables()\n",
    "    gradients = tf.gradients(loss_op,trainable_parameters)\n",
    "    update_step = optimizer.apply_gradients(zip(gradients, trainable_parameters))\n",
    "\n",
    "\n",
    "print('\\n\\nStart of the training...\\n')\n",
    "with tf.Session(graph=main_graph) as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # How many mini-batches in total?\n",
    "    num_batches=int(2048/batch_size)\n",
    "    \n",
    "    loss=0\n",
    "\n",
    "    #Iterate over the entire training set for 10 times\n",
    "    for epoch in range(10):\n",
    "            \n",
    "        # Iterate over the number of mini-batches\n",
    "        for n_batch in range(num_batches-1):\n",
    "            \n",
    "            # Get the next mini-batches of samples for the training set\n",
    "            features, labels=get_next_batch(n_batch)\n",
    "              \n",
    "            # Perform the gradient descent step on that mini-batch and compute the loss value\n",
    "            _, loss_=sess.run((update_step, loss_op), feed_dict={x:features, y:labels})   \n",
    "            loss+=loss_\n",
    "             \n",
    "        print('epoch_nr.: %i, loss: %.3f' %(epoch,(loss/num_batches)))\n",
    "        loss=0 \n",
    "\n",
    "    # Compute the prediction on the last mini-batch\n",
    "    prediction=sess.run(predict_op, feed_dict={x:features})\n",
    "    \n",
    "    # Iterate over the features and labels from the last mini-batch as well as\n",
    "    # the predicitons made by the network, and compare them to check the performance\n",
    "    for f, l, p in zip(features, labels, prediction):\n",
    "        \n",
    "        # Rescale the predictions and labels back into their original value range\n",
    "        p=p*2047\n",
    "        l=l*2047\n",
    "        \n",
    "        print('Binary number: %s,  label: %i, prediciton %i' %(str(f), l,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 regularisation example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generation of Data...\n",
      "\n",
      "binary number: 00000000001, decimal number: 1\n",
      "binary number: 00000000010, decimal number: 2\n",
      "binary number: 00000000011, decimal number: 3\n",
      "binary number: 00000000100, decimal number: 4\n",
      "binary number: 00000000101, decimal number: 5\n",
      "binary number: 00000000110, decimal number: 6\n",
      "binary number: 00000000111, decimal number: 7\n",
      "binary number: 00000001000, decimal number: 8\n",
      "binary number: 00000001001, decimal number: 9\n",
      "binary number: 00000001010, decimal number: 10\n",
      "binary number: 11111110110, decimal number: 2038\n",
      "binary number: 11111110111, decimal number: 2039\n",
      "binary number: 11111111000, decimal number: 2040\n",
      "binary number: 11111111001, decimal number: 2041\n",
      "binary number: 11111111010, decimal number: 2042\n",
      "binary number: 11111111011, decimal number: 2043\n",
      "binary number: 11111111100, decimal number: 2044\n",
      "binary number: 11111111101, decimal number: 2045\n",
      "binary number: 11111111110, decimal number: 2046\n",
      "binary number: 11111111111, decimal number: 2047\n",
      "[[1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1.]\n",
      " [1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      " [0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1.]\n",
      " [1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1.]\n",
      " [1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0.]\n",
      " [1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0.]\n",
      " [0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1.]]\n",
      "[[0.7044455300439668]\n",
      " [0.6902784562774792]\n",
      " [0.7923790913531998]\n",
      " [0.13922813873961895]\n",
      " [0.9379579872984856]\n",
      " [0.8778700537371763]\n",
      " [0.35613092330239376]\n",
      " [0.7801660967269174]\n",
      " [0.7757694186614558]\n",
      " [0.8461162677088422]\n",
      " [0.23546653639472398]\n",
      " [0.1162677088422081]\n",
      " [0.43771372740595993]\n",
      " [0.30434782608695654]\n",
      " [0.040058622374206154]\n",
      " [0.18319491939423546]]\n",
      "\n",
      "\n",
      "Start of the training...\n",
      "\n",
      "epoch_nr.: 0, loss: 5.078\n",
      "epoch_nr.: 1, loss: 3.922\n",
      "epoch_nr.: 2, loss: 3.044\n",
      "epoch_nr.: 3, loss: 2.367\n",
      "epoch_nr.: 4, loss: 1.845\n",
      "epoch_nr.: 5, loss: 1.442\n",
      "epoch_nr.: 6, loss: 1.132\n",
      "epoch_nr.: 7, loss: 0.894\n",
      "epoch_nr.: 8, loss: 0.710\n",
      "epoch_nr.: 9, loss: 0.568\n",
      "Binary number: [1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.],  label: 1839, prediciton 1547\n",
      "Binary number: [1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.],  label: 1542, prediciton 1335\n",
      "Binary number: [1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.],  label: 1154, prediciton 1039\n",
      "Binary number: [0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.],  label: 1007, prediciton 1040\n",
      "Binary number: [1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0.],  label: 1838, prediciton 1542\n",
      "Binary number: [0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1.],  label: 147, prediciton 393\n",
      "Binary number: [0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0.],  label: 894, prediciton 972\n",
      "Binary number: [0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1.],  label: 51, prediciton 333\n",
      "Binary number: [1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.],  label: 2015, prediciton 1645\n",
      "Binary number: [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.],  label: 6, prediciton 286\n",
      "Binary number: [0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0.],  label: 806, prediciton 880\n",
      "Binary number: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0.],  label: 1114, prediciton 1042\n",
      "Binary number: [0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1.],  label: 169, prediciton 414\n",
      "Binary number: [1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0.],  label: 1852, prediciton 1552\n",
      "Binary number: [1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.],  label: 1248, prediciton 1107\n",
      "Binary number: [0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1.],  label: 185, prediciton 446\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "# list that will contain our training set\n",
    "training_data=[]\n",
    "\n",
    "# How many digits should this binary number have?\n",
    "n=11\n",
    "    \n",
    "#Mini-batch size\n",
    "batch_size=16\n",
    "\n",
    "print('\\n\\nGeneration of Data...\\n') \n",
    "for i in np.arange(2048):\n",
    "    \n",
    "    # Create a binary number of type string\n",
    "    b = bin(i)[2:]\n",
    "    l = len(b)\n",
    "    b = str(0) * (n - l) + b  \n",
    "\n",
    "    # Convert the binary string number to type float\n",
    "    features=np.array(list(b)).astype(float)\n",
    "    \n",
    "    # Create and normalize the corresponding decimal label\n",
    "    label=float(i)/2047\n",
    "    \n",
    "    # Put the feature-label instance into the list\n",
    "    training_data.append([features, label])\n",
    "    \n",
    "    if (i>=1 and i<11) or (i>=2038):\n",
    "        print('binary number: %s, decimal number: %d' %(b, i))\n",
    "        \n",
    "# shuffle the data\n",
    "shuffle(training_data)  \n",
    "\n",
    "# convert the list to np.array     \n",
    "training_data=np.array(training_data)\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "# Get the next mini-batch of training samples\n",
    "def get_next_batch(n_batch):\n",
    "    \n",
    "    # Get the next mini-batch of training samples from the dataset\n",
    "    features=training_data[n_batch*batch_size:(batch_size*(1+n_batch)),0]\n",
    "    labels=training_data[n_batch*batch_size:(batch_size*(1+n_batch)),1]\n",
    "    \n",
    "    # Reshape the list of arrays into a nxn np.array\n",
    "    features = np.concatenate(features).reshape([batch_size,11])  \n",
    "    # Reshape the labels \n",
    "    labels=np.reshape(labels, [batch_size,1])\n",
    "    \n",
    "    return features, labels\n",
    "    \n",
    "features, labels=get_next_batch(n_batch=1)\n",
    "\n",
    "print(features)\n",
    "print(labels)\n",
    "#%%  \n",
    "\n",
    "# Create the training graph\n",
    "main_graph=tf.Graph()\n",
    "\n",
    "with main_graph.as_default():\n",
    "    \n",
    "    # Define the placeholders for the features and the labels\n",
    "    x=tf.placeholder(dtype=tf.float32,shape=[batch_size,11], name='features')\n",
    "    y=tf.placeholder(dtype=tf.float32, shape=[batch_size,1], name='labels')\n",
    "           \n",
    "    # Create the weight matrices and the bias vectors \n",
    "    initializer=initializer=tf.random_normal_initializer(mean=0.0, stddev=0.25)\n",
    "   \n",
    "    W1=tf.get_variable('W1',shape=[11,50], initializer=initializer)\n",
    "    W2=tf.get_variable('W2',shape=[50,25], initializer=initializer)\n",
    "    W3=tf.get_variable('W3',shape=[25,1], initializer=initializer)\n",
    "    \n",
    "    b1=tf.get_variable('b1',shape=[50], initializer=initializer)\n",
    "    b2=tf.get_variable('b2',shape=[25], initializer=initializer)\n",
    "\n",
    "    ### Define the forward propagation step ###\n",
    "    \n",
    "    # First hidden layer\n",
    "    z1=tf.matmul(x,W1)+b1\n",
    "    a1=tf.nn.tanh(z1)\n",
    "    \n",
    "    # Second hidden layer\n",
    "    z2=tf.matmul(a1,W2)+b2\n",
    "    a2=tf.nn.tanh(z2)\n",
    "    \n",
    "    # Outputlayer\n",
    "    predict_op=tf.nn.relu(tf.matmul(a2,W3))\n",
    "    \n",
    "\t\n",
    "\t\n",
    "\t#### L2-Regularization ####\n",
    "\t\n",
    "\t# Collect the weight matrices\n",
    "    weight_matrices=[var for var in tf.trainable_variables() if 'bias' not in var.name] \n",
    "\t#Apply the L2 regularization to the weight matrices\n",
    "    l2_losses=[tf.nn.l2_loss(w) for w in weight_matrices]\n",
    "    l2_loss = tf.add_n(l2_losses)\n",
    "    \n",
    "\t# Regularization rate\n",
    "    alpha=0.1\n",
    "    \n",
    "\t# Add the L2 regularization to the regular loss function\n",
    "    loss_op=tf.losses.mean_squared_error(y,predict_op)+alpha*l2_loss\n",
    "\t\n",
    "\t########\n",
    "       \n",
    "    # Perform a gradient descent step\n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    trainable_parameters = tf.trainable_variables()\n",
    "    gradients = tf.gradients(loss_op,trainable_parameters)\n",
    "    update_step = optimizer.apply_gradients(zip(gradients, trainable_parameters))\n",
    "\n",
    "\n",
    "print('\\n\\nStart of the training...\\n')\n",
    "with tf.Session(graph=main_graph) as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # How many mini-batches in total?\n",
    "    num_batches=int(2048/batch_size)\n",
    "    \n",
    "    loss=0\n",
    "\n",
    "    #Iterate over the entire training set for 10 times\n",
    "    for epoch in range(10):\n",
    "            \n",
    "        # Iterate over the number of mini-batches\n",
    "        for n_batch in range(num_batches-1):\n",
    "            \n",
    "            # Get the next mini-batches of samples for the training set\n",
    "            features, labels=get_next_batch(n_batch)\n",
    "              \n",
    "            # Perform the gradient descent step on that mini-batch and compute the loss value\n",
    "            _, loss_=sess.run((update_step, loss_op), feed_dict={x:features, y:labels})   \n",
    "            loss+=loss_\n",
    "             \n",
    "        print('epoch_nr.: %i, loss: %.3f' %(epoch,(loss/num_batches)))\n",
    "        loss=0 \n",
    "\n",
    "    # Compute the prediction on the last mini-batch\n",
    "    prediction=sess.run(predict_op, feed_dict={x:features})\n",
    "    \n",
    "    # Iterate over the features and labels from the last mini-batch as well as\n",
    "    # the predicitons made by the network, and compare them to check the performance\n",
    "    for f, l, p in zip(features, labels, prediction):\n",
    "        \n",
    "        # Rescale the predictions and labels back into their original value range\n",
    "        p=p*2047\n",
    "        l=l*2047\n",
    "        \n",
    "        print('Binary number: %s,  label: %i, prediciton %i' %(str(f), l,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### so after this we will move forward with advance modules of tensorflow\n",
    "### whole purpose of next module is designing ETL (Extract Transformation Load) so that we can fasten training process to next level to increase cpu gpu parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
